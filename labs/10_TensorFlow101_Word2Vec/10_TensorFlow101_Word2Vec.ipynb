{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>TensorFlow 101 & Word2Vec</center>\n",
    "<center>Shan-Hung Wu &amp; DataLab<br/>Fall 2018</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow is a powerful open source libraray used for large-scale machine learning.  \n",
    "In this lab, we will first go through some basic concepts of TensorFlow including graph, session, dataset and tensorboard. At the end, introduce a word2vec model as application to cover all knowledge taught in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup \n",
    "\n",
    "## Installing CUDA and CuDNN\n",
    "In order to use NVIDIA GPUs to train your model, CUDA and CuDNN are required.\n",
    "The installation guide can be found [here](https://www.nvidia.com/en-us/data-center/gpu-accelerated-applications/tensorflow/).\n",
    "\n",
    "## Installing TensorFlow\n",
    "There are several ways to install TensorFlow which can be found [here](https://www.tensorflow.org/install/). One way is to install TensorFlow in a conda virtual environment. First, we create a new environment called `tensorflow`.  \n",
    "```\n",
    "> conda create -n tensorflow\n",
    "```\n",
    "Then we activate the environment:\n",
    "```\n",
    "> source activate tensorflow (Linux or Mac)\n",
    "> activate tensorflow (Windows)\n",
    "```\n",
    "\n",
    "According to the TensorFlow official webpage, it is recommended installing TensorFlow with `pip install` command instead of `conda install`. Since the conda package is community supported, not officially supported, we will stick to `pip install`.  \n",
    "\n",
    "First, make sure that `pip3` is installed:\n",
    "```\n",
    "> pip3 -V\n",
    "```\n",
    "Install TensorFlow with `pip install`:\n",
    "```\n",
    "> pip3 install tensorflow-gpu # Python 3.n; GPU support \n",
    "```\n",
    "Then we can verify the installation by entering a short program in the python interactive shell.\n",
    "```\n",
    "> python\n",
    "```\n",
    "Type in the following program:\n",
    "```python=\n",
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Getting Started with TensorFlow \n",
    "\n",
    "Originally developed by Google Brain, TensorFlow is an open source library which provides a variety of functions and classes used to conduct machine learning.\n",
    "\n",
    "The benefits of using TensorFlow include:\n",
    "- Python API\n",
    "- Portability: can be used on multiple CPUs or GPUs as well as on mobile devices\n",
    "- Flexibility: can run on different devices e.g. Raspberry Pi, Android, iOS, Windows, Linux\n",
    "- Visualization: visualize the training process via TensorBoard\n",
    "- Checkpoints: manage trained models\n",
    "- Auto-differentiation\n",
    "- Large community "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph and Sessions\n",
    "In TensorFlow, the definition of computations is separated from their execution. First, we specify the operations by building a data flow graph in Python. Next, TensorFlow runs the graph with a `Session` using optimized C++ code. Let's import tensorflow first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\" # choose which GPU you want to use\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import urllib\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph \n",
    "A computational graph is a series of TensorFlow operations arranged into a graph. The graph is composed of two types of objects.\n",
    "- `tf.Operation`: The nodes of the graph. Operations describe calculations that consume and produce tensors.\n",
    "- `tf.Tensor`: The edges in the graph. These represent the values that will flow through the graph. Most TensorFlow functions return `tf.Tensors`.\n",
    "\n",
    "> **Notes**: `tf.Tensors` do not have values, they are just handles to elements in the computation graph.\n",
    "\n",
    "### Session\n",
    "To evaluate tensors, instantiate a `tf.Session` object, informally known as a session. A **session** encapsulates the state of the TensorFlow runtime, and runs TensorFlow operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"const3:0\", shape=(), dtype=float32)\n",
      "Tensor(\"const4:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# build a naive graph\n",
    "a_tensor = tf.constant(3., name=\"const3\")\n",
    "b_tensor = tf.constant(4., name=\"const4\")\n",
    "out_tensor = tf.add(a_tensor, b_tensor)\n",
    "print(a_tensor, b_tensor, out_tensor, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that printing the tensors does not output the values `3.0`, `4.0`, and `7.0` as you might expect. The above statements only build the computation graph. These `tf.Tensor` objects just represent the results of the operations that will be run.\n",
    "![tensor_op](assets/tensor_op.png)\n",
    "\n",
    "We need a `tf.Session` to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 3.0 \n",
      "b = 4.0 \n",
      "c = 7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session() # create a session\n",
    "a, b, c = sess.run([a_tensor, b_tensor, out_tensor])\n",
    "print(\"a = {} \\nb = {} \\nc = {}\".format(a, b, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "A **tensor** is a generalization of vectors and matrices to potentially higher dimensions.\n",
    "\n",
    "When writing a TensorFlow program, the main object you manipulate and pass around is the `tf.Tensor`. A `tf.Tensor` object represents a partially defined computation that will eventually produce a value. TensorFlow programs work by first building a graph of `tf.Tensor` objects, detailing how each tensor is computed based on the other available tensors and then by running parts of this graph to achieve the desired results.\n",
    "\n",
    "A `tf.Tensor` has the following properties:\n",
    "- a data type (`tf.float32`, `tf.int32`, or `tf.string`, for example)\n",
    "- a shape\n",
    "\n",
    "The **rank** of a tensor refers to the number of dimensions it has.\n",
    "\n",
    "The **shape** of a tensor speficies the array's length along each dimension.\n",
    "\n",
    "```\n",
    "3. # a rank 0 tensor; a scalar with shape [],\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n",
    "```\n",
    "\n",
    "Some types of tensors are special. <br>\n",
    "The main ones are:\n",
    "- `tf.constant`\n",
    "- `tf.Variable`\n",
    "- `tf.placeholder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.constant\n",
    "We can create constants by passing lists or constants into the `tf.constant` function.\n",
    "```\n",
    "tf.constant(value, dtype=None, shape=None, name='Const', verify_shape=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"vector:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"matrix:0\", shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# constant of 1d tensor (vector)\n",
    "a = tf.constant([2, 2], dtype=tf.int32, name=\"vector\")\n",
    "# constant of 2x2 tensor (matrix)\n",
    "b = tf.constant([[0, 1], [2, 3]], name=\"matrix\")\n",
    "print(a, b, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create tensors of a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"zeros_matrix:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"ones_matrix:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"zeros_like_matrix:0\", shape=(3, 2), dtype=float32)\n",
      "Tensor(\"ones_like_matrix:0\", shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# a matrix with filled zeros\n",
    "c = tf.zeros([2, 3], tf.int32, name=\"zeros_matrix\") # [[0, 0, 0], [0, 0, 0]]\n",
    "# a matrix with filled ones\n",
    "d = tf.ones([2, 3], tf.int32, name=\"ones_matrix\") #  [[1, 1, 1], [1, 1, 1]]\n",
    "\n",
    "# create a tensor filled zeros/ones, with shape and type as input_tensor\n",
    "input_tensor = tf.constant([[1,1], [2,2], [3,3]], dtype=tf.float32)\n",
    "e = tf.zeros_like(input_tensor, name=\"zeros_like_matrix\")  #  [[0, 0], [0, 0], [0, 0]]\n",
    "f = tf.ones_like(input_tensor, name=\"ones_like_matrix\") # [[1, 1], [1, 1], [1, 1]]\n",
    "\n",
    "print(c, d, e, f, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variables\n",
    "\n",
    "Unlike a constant, a variable can be assigned to, so its value can be changed. Also, a constant's value is stored on the graph, whereas a variable's value is stored seperately. To declare a variable, we create a instance of `tf.get_variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'vector_1:0' shape=(3,) dtype=float32_ref>\n",
      "<tf.Variable 'matrix_1:0' shape=(5, 3) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# create a variable of vector\n",
    "vec_var = tf.get_variable(name=\"vector\",\n",
    "                          shape=[3], initializer=tf.ones_initializer)\n",
    "# create a variable of matrix\n",
    "mat_var = tf.get_variable(name=\"matrix\",\n",
    "                          shape=[5, 3], initializer=tf.random_normal_initializer)\n",
    "\n",
    "print(vec_var, mat_var, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can create variables by calling `tf.Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'scalar:0' shape=() dtype=int32_ref>\n",
      "Tensor(\"Assign:0\", shape=(), dtype=int32_ref)\n"
     ]
    }
   ],
   "source": [
    "# instance of `tf.Variable`\n",
    "var = tf.Variable(2, name=\"scalar\")\n",
    "\n",
    "# we can assign new value to a variable\n",
    "var_times_two = var.assign(var * 2) # an operation that assigns value var*2 to var\n",
    "print(var, var_times_two, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f390357fa6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# the following code will casue error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "# constant value is not changable\n",
    "# the following code will casue error\n",
    "c = tf.constant(0.)\n",
    "c.assign(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since constraint mentiond above, all fixed value should be as type `tf.constant`, while trainable weights should be as type `tf.Variable` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use a variable, it must be initialized. If you are programming in the low-level TensorFlow API (that is, you are explicitly creating your own graphs and sessions), you must explicitly initialize the variables.\n",
    "\n",
    "To initialize all trainable variables in one go, before training starts, call `tf.global_variables_initializer()`. This function returns a single operation responsible for initializing all variables in the `tf.GraphKeys.GLOBAL_VARIABLES` collection. Running this operation initializes all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variable_init_op = tf.global_variables_initializer() # an operation\n",
    "sess.run(variable_init_op) # initialize the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.placeholder\n",
    "As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as **placeholders**. A **placeholder** is a promise to provide a value later, like a function argument.\n",
    "```\n",
    "tf.placeholder(dtype, shape=None, name=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"a:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a_placeholder = tf.placeholder(tf.float32, shape=[None, 3], name=\"a\")\n",
    "print(a_placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **placeholder** should be provided a value when executed by `tf.Session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session() \n",
    "a = sess.run(a_placeholder, feed_dict={a_placeholder: [[1, 2, 3], [4, 5, 6]]})\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use an example to summarize above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a basic graph that demos the basic tensorflow concepts\n",
    "with tf.Graph().as_default() as g:\n",
    "    # a constant tensor with rank = 0\n",
    "    scalar_tensor = tf.constant(5., name=\"scalar\") \n",
    "    \n",
    "    # a vector tensor with rank = 1, and filled with random values\n",
    "    vector_tensor = tf.random_normal(shape=[5], name=\"vector\") \n",
    "    \n",
    "    # tensorflow supports broadcast\n",
    "    broadcast_with_scalar = vector_tensor + scalar_tensor \n",
    "    \n",
    "    # use placeholder to get values in runtime\n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, 5], name=\"input\")\n",
    "    feature_dims = x_input.shape[1]\n",
    "    \n",
    "    # a matrix variable with rank = 2.\n",
    "    matrix_variable = tf.get_variable(\"matrix\",\n",
    "                                      shape=[feature_dims, 2],\n",
    "                                      initializer=tf.ones_initializer) \n",
    "    mul_with_matrix = tf.matmul(x_input, matrix_variable, name=\"output\")\n",
    "    \n",
    "    var_init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scalar]\n",
      " 5.0 \n",
      "[vector]\n",
      " [ 0.89785016 -0.23613338 -1.1767508  -0.12170798 -0.46906853] \n",
      "[broadcast]\n",
      " [5.89785   4.7638664 3.8232493 4.878292  4.5309315]\n",
      "[input]\n",
      " [[1. 4. 1. 1. 2.]\n",
      " [1. 4. 1. 3. 2.]\n",
      " [3. 0. 3. 3. 2.]\n",
      " [4. 3. 3. 1. 1.]\n",
      " [0. 1. 4. 3. 1.]] \n",
      "[matrix]\n",
      " [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]] \n",
      "[output]\n",
      " [[ 9.  9.]\n",
      " [11. 11.]\n",
      " [11. 11.]\n",
      " [12. 12.]\n",
      " [ 9.  9.]]\n"
     ]
    }
   ],
   "source": [
    "feed_data = np.random.randint(5, size=[5, 5])\n",
    "\n",
    "# Under the scope of session, we can run the value of tensor in default graph\n",
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(var_init_op) # must initialize the variables\n",
    "    \n",
    "    scalar, vector, broadcast = sess.run([scalar_tensor,\n",
    "                                          vector_tensor,\n",
    "                                          broadcast_with_scalar])\n",
    "    print(\"[scalar]\\n {} \\n[vector]\\n {} \\n[broadcast]\\n {}\".format(scalar,\n",
    "                                                                    vector,\n",
    "                                                                    broadcast))\n",
    "    \n",
    "    x, m, out = sess.run([x_input, matrix_variable, mul_with_matrix],\n",
    "                         feed_dict={x_input: feed_data})\n",
    "    print(\"[input]\\n {} \\n[matrix]\\n {} \\n[output]\\n {}\".format(x, m, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Graph with TensorBoard\n",
    "The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, TF included a suite of visualization tools called **TensorBoard**. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it. \n",
    "![tensorboard](assets/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Graph\n",
    "Create a `writer` with `tf.summary.FileWriter` to write the graph into file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph_dir = \"graphs/demo\"\n",
    "os.makedirs(graph_dir)\n",
    "with tf.Graph().as_default() as g:\n",
    "    const_a = tf.constant(1., shape=[1, 5], name=\"const_a\")\n",
    "    const_b = tf.add(const_a, 5, name=\"const_b\")\n",
    "    var_c = tf.get_variable(\"var_c\", shape=[5, 3])\n",
    "    const_d = tf.matmul(const_b, var_c, name=\"const_d\")\n",
    "    # create a writer \n",
    "    writer = tf.summary.FileWriter(graph_dir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Tensorboard\n",
    "To run TensorBoard, use the following command: <br>\n",
    "> `tensorboard --logdir=path/to/log-directory`\n",
    "\n",
    "where `logdir` points to the directory where the `FileWriter` serialized its data. If this `logdir` directory contains subdirectories which contain serialized data from separate runs, then TensorBoard will visualize the data from all of those runs. Once TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard.\n",
    "![demo-tensorboard](assets/demo-tensorboard.png)\n",
    "#### More\n",
    "TensorBoard provides more than that. You can check [here](https://www.tensorflow.org/guide/summaries_and_tensorboard) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Restore checkpoints\n",
    "Training a deep learning model may take a few hours or even a few days. The learned weights should be saved periodically so that you can restore for further applications. In TensorFlow, all trainable variables can be saved in checkpoints - a binary file that map variable names to tensor values.  \n",
    "In this part, we will guide you how to save and restore the model. TensorFlow provides a superb class `tf.train.Saver` to do this work. Its constructor adds `save` and `restore` ops to the graph for all, or a specified list, of the variables in the graph. The `Saver` object provides methods to run these ops, specifying paths for the checkpoint files to write to or read from.\n",
    "\n",
    "[**Notes**] You can check [here](https://www.tensorflow.org/guide/saved_model) for more than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save variables\n",
    "Create a `Saver` with `tf.train.Saver()` to manage all variables in the model.\n",
    "The following cell shows how to call the `tf.train.Saver.save` method to save variables to checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph]\n",
      "Tensor(\"Const:0\", shape=(5,), dtype=int32)\n",
      "<tf.Variable 'var_b:0' shape=(5,) dtype=int32_ref>\n",
      "Tensor(\"add:0\", shape=(5,), dtype=int32)\n",
      "<tf.Variable 'var_d:0' shape=(3,) dtype=float32_ref>\n",
      "\n",
      "[Trainable variables]\n",
      "<tf.Variable 'var_b:0' shape=(5,) dtype=int32_ref>\n",
      "<tf.Variable 'var_d:0' shape=(3,) dtype=float32_ref>\n",
      "\n",
      "[Value]\n",
      "[2 2 2 2 2]\n",
      "[0 0 0 0 0]\n",
      "[2 2 2 2 2]\n",
      "[1. 1. 1.]\n",
      "\n",
      "[Model saved in path: checkpoints/demo/model.ckpt]\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"checkpoints/demo\"\n",
    "os.makedirs(ckpt_dir)\n",
    "with tf.Graph().as_default() as g:\n",
    "    const_a = tf.constant(2, tf.int32, [5])\n",
    "    var_b = tf.get_variable(\"var_b\", dtype=tf.int32,\n",
    "                            shape=[5], initializer=tf.zeros_initializer) # variable\n",
    "    const_c = var_b + const_a\n",
    "    var_d = tf.get_variable(\"var_d\", shape=[3],\n",
    "                            initializer=tf.ones_initializer) # variable\n",
    "    \n",
    "    print(\"[Graph]\", const_a, var_b, const_c, var_d, sep=\"\\n\")\n",
    "    print(\"\\n[Trainable variables]\", *tf.trainable_variables(), sep=\"\\n\")\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # Declare a saver object to save checkpoints\n",
    "    saver = tf.train.Saver() \n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    # Do some works with the model\n",
    "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
    "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")\n",
    "    \n",
    "    # Save the variables to disk\n",
    "    save_path = saver.save(sess, os.path.join(ckpt_dir, \"model.ckpt\"))\n",
    "    print(\"\\n[Model saved in path: {}]\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect variables in checkpoints \n",
    "This is a useful function to debug. Sometimes you may notice that you have some conflict between your model and checkpoints. That's why you need this function to insepct what you stored in the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_b (DT_INT32) [5]\n",
      "var_d (DT_FLOAT) [3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "# tensor_name: Name of the tensor in the checkpoint file to print. \n",
    "# all_tensors: Boolean indicating whether to print all tensors.\n",
    "# all_tensor_names: Boolean indicating whether to print all tensor names.\n",
    "print_tensors_in_checkpoint_file(save_path, tensor_name=\"\",\n",
    "                                 all_tensors=\"\", all_tensor_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore variables\n",
    "The `tf.train.Saver` object not only saves variables to checkpoint files, it also restores variables. Note that when you restore variables you do not have to initialize them beforehand. For example, the following snippet demonstrates how to call the `tf.train.Saver.restore` method to restore variables from the checkpoint files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/demo/model.ckpt\n",
      "\n",
      "[Value]\n",
      "[2 2 2 2 2]\n",
      "[0 0 0 0 0]\n",
      "[2 2 2 2 2]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    saver.restore(sess, save_path)\n",
    "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
    "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint_path: \"checkpoints/demo/model.ckpt\"\n",
      "all_model_checkpoint_paths: \"checkpoints/demo/model.ckpt\"\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/demo/model.ckpt\n",
      "\n",
      "[Value]\n",
      "[2 2 2 2 2]\n",
      "[0 0 0 0 0]\n",
      "[2 2 2 2 2]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# more robust\n",
    "with tf.Session(graph=g) as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "    print(ckpt)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    a, b, c, d = sess.run([const_a, var_b, const_c, var_d])\n",
    "    print(\"\\n[Value]\", a, b, c, d, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data\n",
    "### Dataset and Iterator \n",
    "The `tf.data` API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The `tf.data` API makes it easy to deal with large amounts of data, different data formats, and complicated transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.data` API introduces two new abstractions to TensorFlow:\n",
    "- A `tf.data.Dataset` represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, an element might be a single training example, with a pair of tensors representing the image data and a label. There are two distinct ways to create a dataset:\n",
    "\n",
    "    - Creating a **source** (e.g. `Dataset.from_tensor_slices()`) constructs a dataset from one or more `tf.Tensor` objects.\n",
    "\n",
    "    - Applying a **transformation** (e.g. `Dataset.batch()`) constructs a dataset from one or more `tf.data.Dataset` objects.\n",
    "\n",
    "\n",
    "- A `tf.data.Iterator` provides the main way to extract elements from a dataset. The operation returned by `Iterator.get_next()` yields the next element of a `Dataset` when executed, and typically acts as the interface between input pipeline code and your model. The simplest iterator is a \"one-shot iterator\", which is associated with a particular `Dataset` and iterates through it once. For more sophisticated uses, the `Iterator.initializer` operation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "## Pseudo dataset\n",
    "dataset_size = 20\n",
    "data = np.random.rand(dataset_size, 5) # 20 examples, each example has 5 features\n",
    "label = np.random.randint(low=0, high=3, size=dataset_size) # this dataset has 3 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original dataset] \n",
      " <TensorSliceDataset shapes: ((5,), ()), types: (tf.float64, tf.int64)>\n",
      "\n",
      "[Transformed dataset] \n",
      " <BatchDataset shapes: ((?, 5), (?,)), types: (tf.float64, tf.int64)>\n",
      "\n",
      "[Iterator] \n",
      " <tensorflow.python.data.ops.iterator_ops.Iterator object at 0x7efff7d78be0>\n",
      "\n",
      "[Elements extracted by iterator] \n",
      " Tensor(\"IteratorGetNext:0\", shape=(?, 5), dtype=float64) \n",
      " Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 7\n",
    "\n",
    "## Create a `dataset` by `from_tensor_slices`\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "print(\"[Original dataset] \\n\", training_dataset)\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "print(\"\\n[Transformed dataset] \\n\", training_dataset)\n",
    "\n",
    "## Create a `iterator` to extract elements from `dataset`\n",
    "training_iterator = training_dataset.make_initializable_iterator()\n",
    "x_input, y_label = training_iterator.get_next()\n",
    "print(\"\\n[Iterator] \\n\", training_iterator)\n",
    "print(\"\\n[Elements extracted by iterator] \\n\", x_input, \"\\n\", y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating tensor of `x_input` and `y_label`, we can build a graph for it.  \n",
    "Here we skip the model building process, we demonstrates how will the tensors display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 batch - 7 examples\n",
      "[[0.1785114  0.37917823 0.97863917 0.9965862  0.17850776]\n",
      " [0.02398287 0.72241064 0.1971222  0.87530128 0.62915861]\n",
      " [0.00299223 0.5728955  0.04305269 0.70530687 0.65392263]\n",
      " [0.0714728  0.14404754 0.41925957 0.47584777 0.94056444]\n",
      " [0.41377267 0.22546002 0.00696483 0.96123835 0.49257399]\n",
      " [0.83134302 0.8257122  0.6612064  0.97110385 0.78329471]\n",
      " [0.09832722 0.91167012 0.40365477 0.53979386 0.6957001 ]] [2 0 2 2 1 0 2]\n",
      "2 batch - 7 examples\n",
      "[[0.30653865 0.25898975 0.13849397 0.93080022 0.2972471 ]\n",
      " [0.23872979 0.94448126 0.06755758 0.30375601 0.94975243]\n",
      " [0.10518478 0.72772465 0.39976475 0.17243995 0.04816434]\n",
      " [0.74252794 0.7780784  0.08125116 0.98228199 0.07168171]\n",
      " [0.87771312 0.91280685 0.30740192 0.29386847 0.3691519 ]\n",
      " [0.19219948 0.97256249 0.29177497 0.57044576 0.27758466]\n",
      " [0.38280096 0.64941004 0.32128166 0.55874674 0.45195284]] [1 2 2 0 2 0 2]\n",
      "3 batch - 6 examples\n",
      "[[0.74044775 0.90206664 0.53942021 0.42437452 0.37208602]\n",
      " [0.66346546 0.77456165 0.13677823 0.62873137 0.45698952]\n",
      " [0.72863796 0.61601528 0.53289283 0.57807317 0.33966963]\n",
      " [0.18878925 0.2962816  0.56295579 0.67741766 0.27760496]\n",
      " [0.72415415 0.5211763  0.20297727 0.69788695 0.41387318]\n",
      " [0.11947213 0.22049638 0.44904797 0.40580298 0.60891654]] [1 0 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True # avoids occupying full memory of GPU\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(training_iterator.initializer) # initialize the iterator \n",
    "    step = 0 # record the steps\n",
    "    try:\n",
    "        while(True):\n",
    "            x_, y_ = sess.run([x_input, y_label])\n",
    "            step += 1\n",
    "            print(\"{} batch - {} examples\".format(step, len(y_)))\n",
    "            print(x_, y_)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the basic mechanics for using `dataset` and `iterator` to consume data. The `tf.data` API provides compact methods to consume data and you can check following link for more comprehensive tutorials. [Importing Data](https://www.tensorflow.org/guide/datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build basic model\n",
    "We have learned essential concepts in TensorFlow. In this part, we will introduce two methods to build a basic model which has one fully connected layer. There are many ways to build a neural network. Here we guide you to use low level and high level method to construct it. Although high level method provides compact utilities in `tf.layers`, it is better to understand the concepts of deep learning model using low level method in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting \n",
    "feature_dims = 784 # example with 784 features\n",
    "neurons = 1024 # fully connected layer with 1024 neurons\n",
    "classes = 10 # 10 classes classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level\n",
    "- Construct the layers in neural network from scratch.\n",
    "- Define `weights` and `bias` as trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(x_inputs, out_dim, name='fc'):\n",
    "    \"\"\" Low level method\n",
    "        x_inputs: a batch examples [batch_size, feature_dims]\n",
    "        out_dim: neurons in this layer.\n",
    "    \"\"\" \n",
    "    in_dim = x_inputs.shape[-1] # feature_dims\n",
    "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "        weights = tf.get_variable(\"weights\", shape=[in_dim, out_dim])\n",
    "        bias = tf.get_variable(\"bias\", shape=[out_dim])\n",
    "        out = tf.matmul(x_inputs, weights) + bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output tensor]\n",
      "Tensor(\"fc/add:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"logits/add:0\", shape=(?, 10), dtype=float32)\n",
      "\n",
      "[Variables] \n",
      "<tf.Variable 'fc/weights:0' shape=(784, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'logits/weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'logits/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, feature_dims])\n",
    "fc = fully_connected_layer(x, neurons, \"fc\")\n",
    "out = fully_connected_layer(fc, classes, \"logits\")\n",
    "print(\"[Output tensor]\", fc, out, sep=\"\\n\")\n",
    "print(\"\\n[Variables] \", *tf.trainable_variables(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level\n",
    "- Construct the layers in neural network using `tf.layers`.\n",
    "- A high level API provided by TensorFlow.\n",
    "- It contains a lot of useful methods and arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output tensor]\n",
      "Tensor(\"fc/add:0\", shape=(?, 1024), dtype=float32)\n",
      "Tensor(\"logits/add:0\", shape=(?, 10), dtype=float32)\n",
      "\n",
      "[Variables] \n",
      "<tf.Variable 'fc/weights:0' shape=(784, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'fc/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'logits/weights:0' shape=(1024, 10) dtype=float32_ref>\n",
      "<tf.Variable 'logits/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, feature_dims])\n",
    "fc = fully_connected_layer(x, neurons, \"fc\")\n",
    "out = fully_connected_layer(fc, classes, \"logits\")\n",
    "print(\"[Output tensor]\", fc, out, sep=\"\\n\")\n",
    "print(\"\\n[Variables] \", *tf.trainable_variables(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model Walkthrough\n",
    "1. Build a **Graph** that:\n",
    "    - Define **dataset** and **iterator**\n",
    "    - Build the **model**\n",
    "    - Define the **loss**\n",
    "    - Define the **optimizer**\n",
    "    - Other tensors or operations you need (optional)\n",
    "\n",
    "2. Execute a **session** to:\n",
    "    - Initialize the variables\n",
    "    - Run the target tensors and operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## template\n",
    "with tf.Graph().as_default() as g:\n",
    "    \"\"\" Define dataset and iterator \"\"\"\n",
    "    with tf.name_scope(\"data\"):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Build the model \"\"\"\n",
    "    with tf.name_scope(\"model\"):\n",
    "        pass\n",
    "\n",
    "    \"\"\" Define the loss \"\"\"\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Define the optimizer \"\"\"\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        pass\n",
    "    \n",
    "    \"\"\" Other tensors or operations you need \"\"\"\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        pass\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    \"\"\" Initialize the variables \"\"\"\n",
    "    \"\"\" Run the target tensors and operations \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word2vec \n",
    "Word2Vec is a computationally-efficient model that learns to embed words into vectors. The goal is to map words that have similar meanings close to each other.\n",
    "\n",
    "## Why represent words as vectors?\n",
    "When dealing with words, a straightforward way would be treating each word as discrete symbols. For instance, `cat` as `2` and `dog` as `1`. However, these symbols carry no information about the original word, making it impossible for us to infer the relationship between cats and dogs (both are four-legged animals and both are pets) based on the symbols alone. Hence, to successfully learn the relationship between them, we might need a large amount of training data.  \n",
    "\n",
    "On the other hand, **Vector space models (VSMs)** which represent words as vectors can help overcome these obstacles. This is based on a key observation that **semantically similar words are often used interchangeably in different contexts**. For example, the words `cat` and `dog` may both appear in a context \"\\_\\_\\_ is my favorate pet.\" When feeding `cat` and `dog` into the NN to predict their nearby words, these two words will be likely to share the same/similar hidden representation in order to predict the same/similar nearby words. \n",
    "\n",
    "## Skip-Gram and CBOW\n",
    "Word2Vec comes in two variants **Skip-Gram** and **CBOW (Continuous Bag-Of-Words)**. Algorithmically, these models are similar. CBOW predicts the target words using its neighborhood(context) whereas Skip-Gram does the inverse, which is to predict context words from the target words. For example, given the sentence `the quick brown fox jumped over the lazy dog`. Defining the context words as the word to the left and right of the target word, CBOW will be trained on the dataset: \n",
    "\n",
    "`([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox)...`  \n",
    "\n",
    "where CBOW tries to predict the target word `quick` from the context words in brackets `[the, brown]`, and predict `brown` from `[quick, fox]` and so on.\n",
    "However, with Skip-Gram, the dataset becomes  \n",
    "\n",
    "`(quick, the), (quick, brown), (brown, quick), (brown, fox), ...`  \n",
    "\n",
    "where Skip-Gram predicts the context word `the`, `brown` with the target word `quick`. Statistically, CBOW smoothes over a lot of the distributional information (by treating an entire context as one example). For the most part, this turns out to be a useful thing for smaller datasets. On the other hand, Skip-Gram treats each context-target pair as a new observation and is shown to be able to capture the semantics better when we have a large dataset.\n",
    "\n",
    "| <img src=\"assets/Skip-gram.png\" width=\"350\"/> | <img src=\"assets/Cbow.png\" width=\"350\"/> |\n",
    "|:---------------------------------------------:|:---------------------------------------------:|\n",
    "|                       Skip-gram               |                 CBOW                      |\n",
    "\n",
    "Note that the tasks described above are only used to train the neural network, we don’t use the neural network for the task we trained it on. What we want is the weights of the hidden layer, the \"embedding matrix\".  \n",
    "\n",
    "For the rest of the tutorial, we will focus on the Skip-Gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Constrative Estimation\n",
    "Before we start implementing a skip-gram model, we want to introduce an important techniques that reduce the computing efforts. It is called **noise constrative estimation**.\n",
    "\n",
    "Let's motivate this idea by a naive method.\n",
    "If we want to create a skip-gram model, we can use the following snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive idea, this is not runnable.\n",
    "\n",
    "vocabulary_size = 10000\n",
    "embedding_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "with tf.Graph().as_default() as g:\n",
    "    center_words = tf.placeholder(tf.int32, [batch_size])\n",
    "    target_words = tf.placeholder(tf.int32, [batch_size])\n",
    "    \n",
    "    encode_matrix = tf.get_variable(\"encoder\", shape=[vocabulary_size, embedding_size])\n",
    "    decode_matrix = tf.get_variable(\"decoder\", shape=[embedding_size, vocabulary_size])\n",
    "    \n",
    "    embedding = tf.matmul(center_words, encode_matrix)\n",
    "    logits = tf.matmul(embedding, decode_matrix)\n",
    "\n",
    "    output = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/softmax-nplm.png\" width=\"350\">\n",
    "As we can see, it has a large number of parameters in the network and the softmax layer is a computationally intensive task. \n",
    "\n",
    "Is there any solution to solve it?\n",
    "\n",
    "**NCE comes to rescue** the problem of softmax.\n",
    "\n",
    "Like most neural networks, a Skip-Gram model is trained using the maximum likelihood(ML) principle:  \n",
    "$$  \\arg\\min_{\\Theta}\\sum_{i=1}^{N}{-\\log\\mathrm{P}(\\boldsymbol{y}^{(i)}\\,|\\,\\boldsymbol{x}^{(i)},\\Theta)} $$\n",
    "In a multiclass task where $y=1,\\cdots,V$($V$ being the vocabulary size) we usually assume\n",
    "\n",
    "$$\\Pr(y\\,|\\,\\boldsymbol{x})\\sim\\mathrm{Categorical}(y\\,|\\,\\boldsymbol{x};\\boldsymbol{\\rho})=\\prod_{i=1}^{V}\\rho_{i}^{1(y;\\,y=i)}.$$\n",
    "\n",
    "It is natural to use $V$ **Softmax units** in the output layer. That is, the activation  $a_i^{(L)}$ of each unit at the last layer(layer $L$) $z_i^{(L)}$ outputs one dimension of the softmax function, a generalization of the logistic sigmoid:\n",
    "\n",
    "\n",
    "$$a_i^{(L)}=\\rho_i=\\mathrm{softmax}(\\boldsymbol{z}^{(L)})_{i}=\\frac{\\exp(z_{i}^{(L)})}{\\sum_{j=1}^{{\\color{red}V}}\\exp(z_{j}^{(L)})}.$$\n",
    "\n",
    "The objective function then becomes:\n",
    "\n",
    "$$\\arg\\min_{\\Theta}\\sum_{i}-\\log\\prod_{j}\\left(\\frac{\\exp(z_{j}^{(L)})}{\\sum_{k=1}^{{\\color{red}V}}\\exp(z_{k}^{(L)})}\\right)^{1(y^{(i)};y^{(i)}=j)}=\\arg\\min_{\\Theta}\\sum_{i}\\left[-z_{y^{(i)}}^{(L)}+\\log\\sum_{k=1}^{{\\color{red}V}}\\exp(z_{k}^{(L)})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, for feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words $w_t$ from $k$ imaginary (noise) words $\\tilde{w}$, in the same context.\n",
    "\n",
    "<img src=\"assets/nce-nplm.png\" width=\"350\">\n",
    "\n",
    "Since we are sampling from two distributions, the correct word is sampled from the true distribution $P$ according to the context $c$ and noise words are sampled from $Q$, which is a noise distribution, in practice, it is said to be a uniform distribution\n",
    "\n",
    "The cost function finally transform to:\n",
    "$$ C(\\theta) = - \\sum_{i=1}^{V} [log\\frac{ exp(\\,z_{i}^{(L)}\\,) }{ exp(\\,z_{i}^{(L)}\\,) + kQ(\\boldsymbol{w})} + logP(1 - \\frac{ exp(\\,z_{i}^{(L)}\\,) }{ exp(\\,z_{i}^{(L)}\\,) + kQ(\\boldsymbol{w})} ] $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Skip gram model with NCE loss\n",
    "### The Dataset\n",
    "The dataset we use is **text8**, which is the first 100 MB of cleaned text of the English Wikipedia dump on Mar. 3, 2006. While 100MB is not enough to train really good embeddings, we can still see some interesting relations. Splitting the text by blank space, we can find that there are 17,005,207 tokens in total.\n",
    "\n",
    "#### Preparing training data\n",
    "To generate batches for training, several functions defined below are used. First, we read the data into the memory and build the vocabulary using a number of most commonly seen words. Meanwhile, we build two dictionaries, a dictionary that translates words to indices and another which does the reverse. Then, for every word in the text selected as the center word, pair them with one of the context words. Finally, a python generator which generates a batch of pairs of center-target pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "# Download the data.\n",
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/'\n",
    "DATA_FOLDER = \"data\"\n",
    "FILE_NAME = \"text8.zip\"\n",
    "EXPECTED_BYTES = 31344016\n",
    "\n",
    "def make_dir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "def download(file_name, expected_bytes):\n",
    "    \"\"\" Download the dataset text8 if it's not already downloaded \"\"\"\n",
    "    local_file_path = os.path.join(DATA_FOLDER, file_name)\n",
    "    if os.path.exists(local_file_path):\n",
    "        print(\"Dataset ready\")\n",
    "        return local_file_path\n",
    "    file_name, _ = urllib.request.urlretrieve(os.path.join(DOWNLOAD_URL, file_name),\n",
    "                                              local_file_path)\n",
    "    file_stat = os.stat(local_file_path)\n",
    "    if file_stat.st_size == expected_bytes:\n",
    "        print('Successfully downloaded the file', file_name)\n",
    "    else:\n",
    "        raise Exception(\n",
    "              'File ' + file_name +\n",
    "              ' might be corrupted. You should try downloading it with a browser.')\n",
    "    return local_file_path    \n",
    "    \n",
    "make_dir(DATA_FOLDER)\n",
    "file_path = download(FILE_NAME, EXPECTED_BYTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# Read the data into a list of strings.\n",
    "def read_data(file_path):\n",
    "    \"\"\" Read data into a list of tokens\"\"\"\n",
    "    with zipfile.ZipFile(file_path) as f:\n",
    "        # tf.compat.as_str() converts the input into the string\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "vocabulary = read_data(file_path)\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "# Build the dictionary and replace rare words with UNK token.\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\" Create two dictionaries and count of occuring words\n",
    "        - word_to_id: map of words to their codes\n",
    "        - id_to_word: maps codes to words (inverse word_to_id)\n",
    "        - count: map of words to count of occurrences\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    word_to_id = dict() # (word, id)\n",
    "    # record word id\n",
    "    for word, _ in count:\n",
    "        word_to_id[word] = len(word_to_id)\n",
    "    id_to_word = dict(zip(word_to_id.values(), word_to_id.keys())) # (id, word)\n",
    "    return word_to_id, id_to_word, count\n",
    "\n",
    "def convert_words_to_id(words, dictionary, count):\n",
    "    \"\"\" Replace each word in the dataset with its index in the dictionary\"\"\"\n",
    "    data_w2id = []\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:\n",
    "            unk_count += 1\n",
    "        data_w2id.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    return data_w2id, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Filling 4 global variables:\n",
    "# data_w2id - list of codes (integers from 0 to vocabulary_size-1).\n",
    "              This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# word_to_id - map of words(strings) to their codes(integers)\n",
    "# id_to_word - maps codes(integers) to words(strings)\n",
    "\"\"\"\n",
    "\n",
    "vocabulary_size = 50000\n",
    "word_to_id, id_to_word, count = build_dataset(vocabulary, vocabulary_size)\n",
    "data_w2id, count = convert_words_to_id(vocabulary, word_to_id, count)\n",
    "del vocabulary  # reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data: {}'.format(data_w2id[:10]))\n",
    "print([id_to_word[i] for i in data_w2id[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function\n",
    "def generate_sample(center_words, context_window_size):\n",
    "    \"\"\" Form training pairs according to the skip-gram model.\"\"\"\n",
    "    for idx, center in enumerate(center_words):\n",
    "        context = random.randint(1, context_window_size)\n",
    "        # get a random target before the center word\n",
    "        for target in center_words[max(0, idx - context) : idx]:\n",
    "            yield center, target\n",
    "        # get a random target after the center word\n",
    "        for target in center_words[idx + 1 : idx + context + 1]:\n",
    "            yield center, target\n",
    "\n",
    "def batch_generator(data, skip_window, batch_size):\n",
    "    \"\"\" Group a numeric stream into batches and yield them as Numpy arrays.\"\"\"\n",
    "    single_gen = generate_sample(data, skip_window)\n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1], dtype=np.int32)\n",
    "        for idx in range(batch_size):\n",
    "            center_batch[idx], target_batch[idx] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## some training settings\n",
    "training_steps = 1000\n",
    "skip_step = 100\n",
    "graph_dir = \"graphs/word2vec_simple\"\n",
    "ckpt_dir = \"checkpoints/word2vec_simple\"\n",
    "\n",
    "## some hyperparameters\n",
    "batch_size = 128\n",
    "embed_size = 128\n",
    "num_sampled = 64\n",
    "learning_rates = 1.0\n",
    "\n",
    "## geneartor for `tf.data.Dataset`\n",
    "def gen():\n",
    "    \"\"\" Return a python generator that generates batches. \"\"\"\n",
    "    yield from batch_generator(data_w2id, 2, batch_size)\n",
    "\n",
    "## model\n",
    "def word2vec(dataset):\n",
    "    \n",
    "    \"\"\" 1. Build the graph\"\"\" \n",
    "    with tf.name_scope(\"data\"):\n",
    "        # one_shot_iterator doesn't need to be initialized\n",
    "        iterator = dataset.make_one_shot_iterator() \n",
    "        center_words, target_words = iterator.get_next() # get the input and output\n",
    "    \n",
    "    with tf.name_scope('embed'):\n",
    "        embedding_matrix = tf.get_variable(\"embedding_matrix\",\n",
    "                                           shape=[vocabulary_size, embed_size])\n",
    "        embedding = tf.nn.embedding_lookup(embedding_matrix,\n",
    "                                           center_words, name='embedding')\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        initializer = tf.truncated_normal_initializer(stddev=1.0 / (embed_size ** 0.5))\n",
    "        nce_weight = tf.get_variable('nce_weight',\n",
    "                                     shape=[vocabulary_size, embed_size],\n",
    "                                     initializer=initializer)\n",
    "        nce_bias = tf.get_variable('nce_bias', shape=[vocabulary_size],\n",
    "                                   initializer=tf.zeros_initializer)\n",
    "\n",
    "        # define loss function to be NCE loss function\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n",
    "                                            biases=nce_bias, \n",
    "                                            labels=target_words, \n",
    "                                            inputs=embedding, \n",
    "                                            num_sampled=num_sampled, \n",
    "                                            num_classes=vocabulary_size), name='loss')\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rates).minimize(loss)\n",
    "    \n",
    "    ## store checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    \"\"\" 2. Execute a session \"\"\"\n",
    "    config = tf.ConfigProto() \n",
    "    config.gpu_options.allow_growth = True  # avoids occupying full memory of GPU\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "\n",
    "        # if that checkpoint exists, restore from checkpoint\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        # we use this to calculate late average loss in the last SKIP_STEP steps\n",
    "        total_loss = 0.0 \n",
    "        writer = tf.summary.FileWriter(graph_dir, sess.graph)\n",
    "\n",
    "        for index in range(1, training_steps+1):\n",
    "            try:\n",
    "                loss_batch, _ = sess.run([loss, optimizer])\n",
    "                total_loss += loss_batch\n",
    "                if index % skip_step == 0:\n",
    "                    print('Average loss at step {}: {:5.1f}'.format(\n",
    "                        index, total_loss/skip_step))\n",
    "                    total_loss = 0.0\n",
    "                    saver.save(sess, os.path.join(ckpt_dir, \"model\"), index)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 100: 234.9\n",
      "Average loss at step 200: 187.4\n",
      "Average loss at step 300: 162.8\n",
      "Average loss at step 400: 148.5\n",
      "Average loss at step 500: 142.2\n",
      "Average loss at step 600: 131.5\n",
      "Average loss at step 700: 122.0\n",
      "Average loss at step 800: 112.1\n",
      "Average loss at step 900: 111.5\n",
      "Average loss at step 1000: 104.3\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(gen,\n",
    "                                         (tf.int32, tf.int32),\n",
    "                                         (tf.TensorShape([batch_size]),\n",
    "                                          tf.TensorShape([batch_size, 1])))\n",
    "word2vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "- Submit to iLMS. Filename: `Lab10_{student_id}.ipynb`\n",
    "- Deadline: 2018/10/25 15:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning the XOR\n",
    "\n",
    "**[Problem Definition]**\n",
    "- Build a model to challenge this task.\n",
    "- Input: binary data of shape=`[2]`\n",
    "- Output: xor result of this input, shape=`[1]`\n",
    "\n",
    "**[Requirements]**\n",
    "- Show the code of **graph** and **session** you build. \n",
    "- Use **low level API method** (define the weights and bias from scratch like above)\n",
    "- Show the accuracy of these 4 examples (it should be `100%`) \n",
    "- Show the **values of weights and bias** you used.\n",
    "\n",
    "**[Notes]**\n",
    "- The model architecture are not constrainted.\n",
    "- It is an easy nonlinear function that neural network can fit rapidly.\n",
    "\n",
    "<img src=\"assets/xor.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xor task\n",
    "xor_data = np.array([[1, 0],\n",
    "                    [0, 1],\n",
    "                    [1, 1],\n",
    "                    [0, 0]])\n",
    "xor_label = np.array([[1], [1], [0], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    x_input = tf.placeholder(tf.float32, [None, 2])\n",
    "    y_label = tf.placeholder(tf.float32, [None, 1])\n",
    "    \n",
    "    # start building your model and meet the requirements\n",
    "    # from here\n",
    "    \n",
    "with tf.Session(graph=g) as sess:\n",
    "    # start run the seesion and meet the requrements\n",
    "    # from here \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. MNIST - Digits classification\n",
    "**[Problem Definition]**\n",
    "- Build a **fully connected network** to challenge this task.\n",
    "- Input: an image of shape=`[784]`\n",
    "- Output: a digit of this image, shape=`[10]`\n",
    "\n",
    "\n",
    "**[Requirements]**\n",
    "- Show the code of **graph** and **session** you build.\n",
    "- Use `tf.data.Dataset` and `tf.data.Iterator` to extract data.\n",
    "- Use low level API method to build the model (define the weights and bias from scratch like above).\n",
    "- The accuracy on `mnist.test` should be at least `95%`.\n",
    "\n",
    "\n",
    "**[Notes]**  \n",
    "- the **hyperparameters** are **not constrainted** (e.g. `num_neurons_in_one_layer`, `how_many_layers`, `learning_rates`, `training_epochs`, `batch_size`)  \n",
    "- the **optimizer** are **not constrainted**.\n",
    "- `mnist.train` is all you can use to train the model. `mnist.validation` are just used to be validated.\n",
    "\n",
    "<img src=\"assets/fc.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# load mnist data\n",
    "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 55000\n",
      "validation data size: 5000\n",
      "testing data size: 10000\n",
      "Shape of image: (784,)\n",
      "Shape of label: (10,)\n"
     ]
    }
   ],
   "source": [
    "print(\"training data size: {}\".format(mnist.train.num_examples))\n",
    "print(\"validation data size: {}\".format(mnist.validation.num_examples))\n",
    "print(\"testing data size: {}\".format(mnist.test.num_examples))\n",
    "\n",
    "print(\"Shape of image: {}\".format(mnist.train.images[0].shape))\n",
    "print(\"Shape of label: {}\".format(mnist.train.labels[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/0AAADKCAYAAAD3qCDoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFTq/wCp0eskqzMwNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x1296 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 5\n",
    "plt.figure(figsize=(18,18))\n",
    "for i in range(5):\n",
    "    plt.subplot(n, n, i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='gray')\n",
    "    plt.title(np.argmax(mnist.train.labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    ## meet the requirements\n",
    "    ## then you will\n",
    "                    pass\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    ## just do it\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
