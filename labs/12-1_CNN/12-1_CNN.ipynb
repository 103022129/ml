{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Nuts and Bolts of Convolution Neural Networks</center>\n",
    "<center>\n",
    "        \"Shan-Hung Wu & DataLab\"\n",
    "        <br>\n",
    "        \"Fall 2018\"\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we introduce two datasets, **mnist** and **cifar**, then we will talk about how to implement CNN models for these two datasets using tensorflow. The major difference between mnist and cifar is their size. Due to the limit of memory size and time issue, we offer a guide to illustrate typical **input pipeline** of tensorflow. Let's dive into tensorflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from a simple dataset. MNIST is a simple computer vision dataset. It consists of images of handwritten digits like:\n",
    "\n",
    "<center><img style='width: 30%' src='imgsrc/MNIST.png' /></center>\n",
    "\n",
    "It also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1. Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers:\n",
    "\n",
    "<center><img style='width: 30%' src='imgsrc/MNIST2.png' /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST data is hosted on [Yann LeCun's website](http://yann.lecun.com/exdb/mnist/). We can directly import MNIST dataset from Tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hcgogogo/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting dataset/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "dest_directory = 'dataset/mnist'\n",
    "# check the directory\n",
    "if not os.path.exists(dest_directory):\n",
    "  os.makedirs(dest_directory)\n",
    "# import data\n",
    "mnist = input_data.read_data_sets(\"dataset/mnist/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping to *Convolutional Neural Network* model, we're going to start with a very simple model with a single layer and softmax regression.\n",
    "\n",
    "We know that every image in MNIST is a handwritten digit between zero and nine. So there are only ten possible digits that a given image can be. We want to give the probability of the input image for being each digit. That is, input an image, the model outputs a ten-dimension vector.\n",
    "\n",
    "This is a classic case where a softmax regression is a natural, simple model. If you want to assign probabilities to an object being one of several different things, softmax is the thing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(x_inputs, out_dim, name='fc'):\n",
    "  \"\"\" \n",
    "      x_inputs: a batch examples [batch_size, feature_dims]\n",
    "      out_dim: neurons in this layer.\n",
    "  \"\"\" \n",
    "  in_dim = x_inputs.shape[-1] # feature_dims\n",
    "  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "      weights = tf.get_variable(\"weights\", shape=[in_dim, out_dim])\n",
    "      bias = tf.get_variable(\"bias\", shape=[out_dim])\n",
    "      out = tf.matmul(x_inputs, weights) + bias\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST_model_fully(object):\n",
    "  def __init__(self, dataset, hps):\n",
    "    self.dataset = dataset\n",
    "    #setting hyperparameters\n",
    "    self.training_steps = hps.steps\n",
    "    self.batch_size = hps.steps\n",
    "    self.input_dim = hps.input_dim\n",
    "    self.output_dim = hps.output_dim\n",
    "    #define weights and build model architecture\n",
    "    self._build_model()\n",
    "    \n",
    "  def _build_model(self):\n",
    "    with tf.name_scope(\"input\") as scope:\n",
    "      self.x = tf.placeholder(tf.float32,[None, self.input_dim])  # flatten into vector of 28 x 28 = 784\n",
    "      self.y_true = tf.placeholder(tf.float32, [None, self.output_dim])  # true answers\n",
    "\n",
    "    with tf.name_scope(\"network\") as scope:\n",
    "      self.y_pred = fully_connected_layer(self.x, self.output_dim, name='out')\n",
    "\n",
    "    with tf.name_scope(\"loss_func\") as scope:\n",
    "      self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_true, \n",
    "                                                                                  logits=self.y_pred))\n",
    "      correct_prediction = tf.equal(tf.argmax(self.y_pred, 1), tf.argmax(self.y_true, 1))\n",
    "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"train_op\") as scope:\n",
    "      self.train_op = tf.train.GradientDescentOptimizer(0.5).minimize(self.cross_entropy)  \n",
    "      \n",
    "  def train(self):\n",
    "    for step in range(self.training_steps):\n",
    "      batch_x, batch_y = self.dataset.train.next_batch(self.batch_size)\n",
    "      self.sess.run(self.train_op, feed_dict = {\n",
    "          self.x: batch_x,\n",
    "          self.y_true: batch_y\n",
    "        })\n",
    "  def test(self):\n",
    "    test_x, test_y = self.dataset.test.images, self.dataset.test.labels\n",
    "    test_accuracy = self.sess.run(self.accuracy, feed_dict = {\n",
    "          self.x: test_x, \n",
    "          self.y_true: test_y\n",
    "      })\n",
    "    print(\"Testing Accuracy : %.3f\" %test_accuracy)\n",
    "  def run(self):\n",
    "    #define session and initialize variables\n",
    "    self.sess = tf.Session()\n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "    self.train()\n",
    "    self.test()\n",
    "    self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```tf.contib.training.HParams``` can help you control your hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_hps= tf.contrib.training.HParams(\n",
    "  steps = 1000, \n",
    "  batch_size = 32,\n",
    "  input_dim = 784,\n",
    "  output_dim = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our model and defining the loss and optimizer, we can start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 0.921\n"
     ]
    }
   ],
   "source": [
    "mnist_fc_model = MNIST_model_fully(mnist, model_hps)\n",
    "mnist_fc_model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the above result, we got about 92% accuracy for *Softmax Regression* on MNIST. In fact, it's not so good. This is because we're using a very simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Convolutional Network on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now jumping from a very simple model to something moderately sophisticated: a small *Convolutional Neural Network*. This will get us to around 99.2% accuracy, not state of the art, but respectable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the diagram of the model we're going to build:\n",
    "\n",
    "<center><img style='width: 30%' src='imgsrc/mnist_deep.png' /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow gives us a lot of flexibility in **convolution** and **pooling** operations. How do we handle the boundaries? What is our stride size? For now, we're going to choose the vanilla version. To keep our code cleaner, let's also abstract those operations into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input.\n",
    "# Our pooling is plain old max pooling over 2x2 blocks.\n",
    "def conv_block(x, name, kernel_width, kernel_height, inp_channel, out_channel, strides = [1, 1, 1, 1], padding='SAME'):\n",
    "  W_conv = tf.get_variable(name+'w', [kernel_width, kernel_height, inp_channel, out_channel])\n",
    "  b_conv = tf.get_variable(name+'b', [out_channel])\n",
    "  return tf.nn.relu(tf.nn.conv2d(x, W_conv, strides=strides, padding=padding)+b_conv)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(\n",
    "      x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MNIST_model_cnn(object):\n",
    "  def __init__(self, dataset, hps):\n",
    "    self.dataset = dataset\n",
    "    #setting hyperparameters\n",
    "    self.training_steps = hps.steps\n",
    "    self.batch_size = hps.steps\n",
    "    self.image_size = hps.image_size\n",
    "    self.output_dim = hps.output_dim\n",
    "    self.dropout_keep_prob = hps.dropout_keep_prob\n",
    "    #define weights and build model architecture\n",
    "    self._build_model()\n",
    "    \n",
    "  def _build_model(self):\n",
    "    with tf.name_scope(\"input\") as scope:\n",
    "      self.x = tf.placeholder(tf.float32,[None, self.image_size*self.image_size*1])  # flatten into vector of 28 x 28 = 784\n",
    "      self.y_true = tf.placeholder(tf.float32, [None, self.output_dim])  # true answers\n",
    "\n",
    "    with tf.name_scope(\"network\") as scope:\n",
    "      # self.y_pred = fully_connected_layer(self.x, self.output_dim, name='out')\n",
    "      x_image = tf.reshape(self.x, [-1, self.image_size, self.image_size, 1])\n",
    "      # 1st convolutional layer\n",
    "      # cnn kernel : [5, 5, 1] *32, output shape = [batch, 28, 28, 32]\n",
    "      h_conv1 = conv_block(x_image, \"conv1\", 5, 5, 1, 32)\n",
    "      # 2x2 pooling, output shape = [batch, 14, 14, 32]\n",
    "      h_pool1 = max_pool_2x2(h_conv1)\n",
    "      \n",
    "      # 2nd convolutional layer\n",
    "      h_conv2 = conv_block(h_pool1, \"conv2\", 5, 5, 32, 64)\n",
    "      # 2x2 pooling, output shape = [batch, 7, 7, 64]\n",
    "      h_pool2 = max_pool_2x2(h_conv2)\n",
    "      \n",
    "      # Densely connected layer\n",
    "      # Flatten the feature maps into [batch, 7 x 7 x 64]\n",
    "      h_pool2_flat = tf.reshape(h_pool2, [-1, self.image_size//4 * self.image_size//4 * 64]) \n",
    "      \n",
    "      # We add a fully-connected layer with 1024 neurons to allow processing on the entire image.\n",
    "      h_fc1 = tf.nn.relu(fully_connected_layer(h_pool2_flat, 1024, name='fc1'))\n",
    "      \n",
    "      # Dropout, this can prevent overfitting\n",
    "      self.keep_prob = tf.placeholder(tf.float32)\n",
    "      h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n",
    "      \n",
    "      # Finally, we add the output layer, just like for the one layer softmax regression above.\n",
    "      self.y_pred = fully_connected_layer(h_fc1_drop, self.output_dim, name='out')\n",
    "      \n",
    "    # After defining our model, we then define our loss and optimizer.\n",
    "    with tf.name_scope(\"loss_func\") as scope:\n",
    "      self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_true, \n",
    "                                                                                  logits=self.y_pred))\n",
    "      correct_prediction = tf.equal(tf.argmax(self.y_pred, 1), tf.argmax(self.y_true, 1))\n",
    "      self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.name_scope(\"train_op\") as scope:\n",
    "      self.train_op = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "      \n",
    "  def train(self):\n",
    "    for step in range(self.training_steps):\n",
    "      batch_x, batch_y = self.dataset.train.next_batch(self.batch_size)\n",
    "      _, accuracy = self.sess.run((self.train_op, self.accuracy), feed_dict = {\n",
    "          self.x: batch_x,\n",
    "          self.y_true: batch_y,\n",
    "          self.keep_prob: self.dropout_keep_prob\n",
    "        })\n",
    "      if step % 200 == 0:\n",
    "        print(\"Step %d, Training Accuracy: %.3f\" %(step, accuracy))\n",
    "        \n",
    "    print(\"Step %d, Training Accuracy: %.3f\" %(step, accuracy))\n",
    "        \n",
    "  def test(self):\n",
    "    test_x, test_y = self.dataset.test.images, self.dataset.test.labels\n",
    "    test_accuracy = self.sess.run(self.accuracy, feed_dict = {\n",
    "          self.x: test_x, \n",
    "          self.y_true: test_y, \n",
    "          self.keep_prob: 1.0 #when testing, we don't apply dropout\n",
    "      })\n",
    "    print(\"Testing Accuracy : %.3f\" %test_accuracy)\n",
    "  def run(self):\n",
    "    #define session and initialize variables\n",
    "    self.sess = tf.Session()\n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "    self.train()\n",
    "    self.test()\n",
    "    self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce overfitting, we will apply [*dropout*](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) before the readout layer. The idea behind dropout is to train an ensemble of model instead of a single model. During training, we drop out neurons with probability $p$, i.e., the probability to keep is $1-p$. When a neuron is dropped, its output is set to zero. These dropped neurons do not contribute to the training phase in forward pass and backward pass. For each training phase, we train the network slightly different from the previous one. It's just like we train different networks in each training phrase. However, during testing phase, we **don't** drop any neuron, and thus, implement dropout is kind of like doing ensemble. Also, randomly drop units in training phase can prevent units from co-adapting too much. Thus, dropout is a powerful regularization techique to deal with *overfitting*. \n",
    " \n",
    "We create a placeholder for the probability that a neuron's output is kept during dropout. This allows us to turn dropout on during training, and turn it off during testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well does this model do! Note that we will include the additional parameter **keep_prob** in feed_dict to control the dropout rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_hps_cnn = tf.contrib.training.HParams(\n",
    "  steps = 2000, \n",
    "  batch_size = 32,\n",
    "  image_size = 28,\n",
    "  output_dim = 10,\n",
    "  dropout_keep_prob = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "CNN_model = MNIST_model_cnn(mnist, model_hps_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Training Accuracy: 0.113\n",
      "Step 200, Training Accuracy: 0.929\n",
      "Step 400, Training Accuracy: 0.974\n",
      "Step 600, Training Accuracy: 0.982\n",
      "Step 800, Training Accuracy: 0.986\n",
      "Step 1000, Training Accuracy: 0.989\n",
      "Step 1200, Training Accuracy: 0.990\n",
      "Step 1400, Training Accuracy: 0.993\n",
      "Step 1600, Training Accuracy: 0.993\n",
      "Step 1800, Training Accuracy: 0.992\n",
      "Step 1999, Training Accuracy: 0.993\n",
      "Testing Accuracy : 0.991\n"
     ]
    }
   ],
   "source": [
    "CNN_model.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final testing accuracy should be approximately 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually MNIST is a easy dataset for the beginner. To demonstrate the power of *Neural Networks*, we need a larger dataset *CIFAR-10*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Here are the classes in the dataset, as well as 10 random images from each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style='width: 40%' src='imgsrc/CIFAR10.png' /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before jumping to a complicated neural network model, we're going to start with **KNN** and **SVM**. The motivation here is to compare neural network model with traditional classifiers, and highlight the performance of neural network model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers convenient facilities that automatically download some well-known datasets and store them in the ~/.keras/datasets directory. Let's load the CIFAR-10 in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 97s 1us/step\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# convert class vectors to binary vectors\n",
    "Y_train = np_utils.to_categorical(y_train)\n",
    "Y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are loaded as integers, so we need to cast it to floating point values in order to perform the division:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we also convert the images into the grayscale. We use the [Luma coding](https://en.wikipedia.org/wiki/Grayscale#Luma_coding_in_video_systems) that is common in video systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAACICAYAAAABDZUdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf5161fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# transform an 3-channel image into one channel\n",
    "def grayscale(data, dtype='float32'):\n",
    "  # luma coding weighted average in video systems\n",
    "  r = np.asarray(.3, dtype=dtype)\n",
    "  g = np.asarray(.59, dtype=dtype)\n",
    "  b = np.asarray(.11, dtype=dtype)\n",
    "  rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
    "  # add channel dimension\n",
    "  rst = np.expand_dims(rst, axis=3)\n",
    "  return rst\n",
    "\n",
    "X_train_gray = grayscale(X_train)\n",
    "X_test_gray = grayscale(X_test)\n",
    "\n",
    "# plot a randomly chosen image\n",
    "img = round(np.random.rand() * X_train.shape[0])\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X_train[img], interpolation='none')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(\n",
    "    X_train_gray[img, :, :, 0], cmap=plt.get_cmap('gray'), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the objects in grayscale images can still be recognizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "When coming to object detection, HOG (histogram of oriented gradients) is often extracted as a feature for classification. It first calculates the gradients of each image patch using sobel filter, then use the magnitudes and orientations of derived gradients to form a histogram per patch (a vector). After normalizing these histograms, it concatenates them into one HOG feature. For more details, read this [tutorial](https://www.learnopencv.com/histogram-of-oriented-gradients/). \n",
    ">Note.  one can directly feed the original images for classification; however, it will take lots of time to train and get worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The code is credit to: \"http://www.itdadao.com/articles/c15a1243072p0.html\"\n",
    "def getHOGfeat(image,\n",
    "               stride=8,\n",
    "               orientations=8,\n",
    "               pixels_per_cell=(8, 8),\n",
    "               cells_per_block=(2, 2)):\n",
    "  cx, cy = pixels_per_cell\n",
    "  bx, by = cells_per_block\n",
    "  sx, sy, sz = image.shape\n",
    "  n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
    "  n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
    "  n_blocksx = (n_cellsx - bx) + 1\n",
    "  n_blocksy = (n_cellsy - by) + 1\n",
    "  gx = np.zeros((sx, sy), dtype=np.double)\n",
    "  gy = np.zeros((sx, sy), dtype=np.double)\n",
    "  eps = 1e-5\n",
    "  grad = np.zeros((sx, sy, 2), dtype=np.double)\n",
    "  for i in range(1, sx - 1):\n",
    "    for j in range(1, sy - 1):\n",
    "      gx[i, j] = image[i, j - 1] - image[i, j + 1]\n",
    "      gy[i, j] = image[i + 1, j] - image[i - 1, j]\n",
    "      grad[i, j, 0] = np.arctan(gy[i, j] / (gx[i, j] + eps)) * 180 / math.pi\n",
    "      if gx[i, j] < 0:\n",
    "        grad[i, j, 0] += 180\n",
    "      grad[i, j, 0] = (grad[i, j, 0] + 360) % 360\n",
    "      grad[i, j, 1] = np.sqrt(gy[i, j]**2 + gx[i, j]**2)\n",
    "  normalised_blocks = np.zeros((n_blocksy, n_blocksx, by * bx * orientations))\n",
    "  for y in range(n_blocksy):\n",
    "    for x in range(n_blocksx):\n",
    "      block = grad[y * stride:y * stride + 16, x * stride:x * stride + 16]\n",
    "      hist_block = np.zeros(32, dtype=np.double)\n",
    "      eps = 1e-5\n",
    "      for k in range(by):\n",
    "        for m in range(bx):\n",
    "          cell = block[k * 8:(k + 1) * 8, m * 8:(m + 1) * 8]\n",
    "          hist_cell = np.zeros(8, dtype=np.double)\n",
    "          for i in range(cy):\n",
    "            for j in range(cx):\n",
    "              n = int(cell[i, j, 0] / 45)\n",
    "              hist_cell[n] += cell[i, j, 1]\n",
    "          hist_block[(k * bx + m) * orientations:(k * bx + m + 1) * orientations] = hist_cell[:]\n",
    "      normalised_blocks[y, x, :] = hist_block / np.sqrt(\n",
    "          hist_block.sum()**2 + eps)\n",
    "  return normalised_blocks.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our *getHOGfeat* function, we then get the HOG features of all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take some minutes.\n"
     ]
    }
   ],
   "source": [
    "X_train_hog = []\n",
    "X_test_hog = []\n",
    "\n",
    "print('This will take some minutes.')\n",
    "\n",
    "for img in X_train_gray:\n",
    "  img_hog = getHOGfeat(img)\n",
    "  X_train_hog.append(img_hog)\n",
    "\n",
    "for img in X_test_gray:\n",
    "  img_hog = getHOGfeat(img)\n",
    "  X_test_hog.append(img_hog)\n",
    "\n",
    "X_train_hog_array = np.asarray(X_train_hog)\n",
    "X_test_hog_array = np.asarray(X_test_hog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors (KNN) on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[scikit-learn](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) provides off-the-shelf libraries for classification. For KNN and SVM classifiers, we can just import from scikit-learn to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KNN]\n",
      "Misclassified samples: 5334\n",
      "Accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# p=2 and metric='minkowski' means the Euclidean Distance\n",
    "knn = KNeighborsClassifier(n_neighbors=11, p=2, metric='minkowski')\n",
    "\n",
    "knn.fit(X_train_hog_array, y_train.ravel())\n",
    "y_pred = knn.predict(X_test_hog_array)\n",
    "print('[KNN]')\n",
    "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the accuracy of KNN on CIFAR-10 is embarrassingly bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM) on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear SVC]\n",
      "Misclassified samples: 4940\n",
      "Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "# C is the hyperparameter for the error penalty term\n",
    "# gamma is the hyperparameter for the rbf kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=0, gamma=0.2, C=10.0)\n",
    "\n",
    "svm_linear.fit(X_train_hog_array, y_train.ravel())\n",
    "y_pred = svm_linear.predict(X_test_hog_array)\n",
    "print('[Linear SVC]')\n",
    "print('Misclassified samples: %d' % (y_test.ravel() != y_pred).sum())\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test.ravel(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By above, SVM is slightly better than KNN, but still poor. Next, we'll design a CNN model using tensorflow. Because the cifar10 is not a small dataset, we can't just use feed_dict to feed all training data to the model due to the limit of memory size. Even if we can feed all data into the model, we still want the process of loading data is efficient. **Input pipeline** is the common way to solve these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline\n",
    "\n",
    "### Queues\n",
    "Because tf.Session objects are designed to be **multithreaded** and thread-safe, so multiple threads can easily use the same session and run ops in parallel. [Queues](https://www.tensorflow.org/programmers_guide/threading_and_queues) are useful because of the ability to **compute tensor asynchronously** in a graph. Most of the time, we use queues to handle inputs. In this way, multiple threads prepare training example and enequeue these examples. In addition, only parts of inputs would be read into memory a time, instead of all of them. This can avoid **out of memory error** when data is large.\n",
    "\n",
    "\n",
    "### Typical Input Pipeline\n",
    "1. The list of filenames\n",
    "2. Optional filename shuffling\n",
    "3. Optional epoch limit\n",
    "4. Filename queue\n",
    "5. A Reader for the file format\n",
    "6. A decoder for a record read by the reader\n",
    "7. Optional preprocessing\n",
    "8. Example queue  \n",
    "\n",
    "<center><img style='width: 70%' src='imgsrc/AnimatedFileQueues.gif' /></center>  \n",
    "\n",
    "We've specified the order of input pipeline in the followng codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data Manually\n",
    "To know how it works under the hood, let's load CIFAR-10 by our own (not using keras). According the descripion, the dataset file is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. We define some constants based on the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the url to download CIFAR-10 dataset (binary version)\n",
    "# see format and details here: http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "DEST_DIRECTORY = 'dataset/cifar10'\n",
    "# the image size we want to keep\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "BATCH_SIZE = 128\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10 \n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Done\n"
     ]
    }
   ],
   "source": [
    "def maybe_download_and_extract(dest_directory, url):\n",
    "  \"\"\"\n",
    "    Download the dataset and extract the data\n",
    "  \"\"\"\n",
    "    \n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  file_name = 'cifar-10-binary.tar.gz'\n",
    "  file_path = os.path.join(dest_directory, file_name)\n",
    "  # if have not downloaded yet\n",
    "  if not os.path.exists(file_path):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r%.1f%%' % \n",
    "            (float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()  # flush the buffer\n",
    "\n",
    "    print('>> Downloading %s ...' % file_name)\n",
    "    file_path, _ = urllib.request.urlretrieve(url, file_path, _progress)\n",
    "    file_size = os.stat(file_path).st_size\n",
    "    print('\\r>> Total %d bytes' % file_size)\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    # Open for reading with gzip compression, then extract all\n",
    "    tarfile.open(file_path, 'r:gz').extractall(dest_directory)\n",
    "  print('>> Done')\n",
    "\n",
    "# download it\n",
    "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset, we create functions\n",
    "* ```distort_input(training_file, batch_size)``` to get a training example queue.\n",
    "* ```eval_input(testing_file, batch_size)``` to get a testing example queue.\n",
    "* ```read_cifar10(filename_queue)``` to read a record from dataset with a filename queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the folder store the dataset\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "# (1) a list of training/testing filenames\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1,6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (5) + (6)\n",
    "def read_cifar10(filename_queue):\n",
    "  \"\"\" Reads and parses examples from CIFAR10 data files.\n",
    "    -----\n",
    "    Args:\n",
    "        filename_queue: \n",
    "            A queue of strings with the filenames to read from.\n",
    "    Returns:\n",
    "        An object representing a single example, with the following fields:\n",
    "        height: \n",
    "            number of rows in the result (32)\n",
    "        width: \n",
    "            number of columns in the result (32)\n",
    "        depth: \n",
    "            number of color channels in the result (3)\n",
    "        key: \n",
    "            a scalar string Tensor describing the filename & record number for this example.\n",
    "        label: \n",
    "            an int32 Tensor with the label in the range 0..9.\n",
    "        image: \n",
    "            a [height, width, depth] uint8 Tensor with the image data\n",
    "  \"\"\"\n",
    "\n",
    "  class CIFAR10Record(object):\n",
    "    pass\n",
    "\n",
    "  result = CIFAR10Record()\n",
    "  # CIFAR10 consists of 60000 32x32 'color' images in 10 classes\n",
    "  label_bytes = 1  # 10 class\n",
    "  result.height = IMAGE_HEIGHT\n",
    "  result.width = IMAGE_WIDTH\n",
    "  result.depth = IMAGE_DEPTH\n",
    "  image_bytes = result.height * result.width * result.depth\n",
    "  # bytes of a record: label(1 byte) followed by pixels(3072 bytes)\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "  # (5) reader for cifar10 file format\n",
    "  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
    "  # read a record\n",
    "  result.key, record_string = reader.read(filename_queue)\n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  # (6) decoder\n",
    "  record_uint8 = tf.decode_raw(record_string, tf.uint8)\n",
    "  # get the label and cast it to int32\n",
    "  result.label = tf.cast(\n",
    "      tf.strided_slice(record_uint8, [0], [label_bytes]), tf.int32)\n",
    "  # [depth, height, width], uint8\n",
    "  depth_major = tf.reshape(\n",
    "      tf.strided_slice(record_uint8, [label_bytes],\n",
    "                       [label_bytes + image_bytes]),\n",
    "      [result.depth, result.height, result.width])\n",
    "  # change to [height, width, depth], uint8\n",
    "  result.image = tf.transpose(depth_major, [1, 2, 0])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distort_input(training_files, batch_size):\n",
    "  \"\"\" Construct distorted input for CIFAR training using the Reader ops.\n",
    "    -----\n",
    "    Args:\n",
    "        training_files: \n",
    "            an array of paths of the training files.\n",
    "        batch_size: \n",
    "            Number of images per batch.\n",
    "    Returns:\n",
    "        images: Images. \n",
    "            4D tensor of [batch_size, IMAGE_SIZE, IMAGE_SIZE, 3] size.\n",
    "        labels: Labels. \n",
    "            1D tensor of [batch_size] size.\n",
    "  \"\"\"\n",
    "  for f in training_files:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "  # create a queue that produces filenames to read\n",
    "  # (4) filename queue\n",
    "  file_queue = tf.train.string_input_producer(training_files)\n",
    "  # (5) + (6)\n",
    "  cifar10_record = read_cifar10(file_queue)\n",
    "  # (7) image preprocessing for training\n",
    "  height = IMAGE_SIZE_CROPPED\n",
    "  width = IMAGE_SIZE_CROPPED\n",
    "  float_image = tf.cast(cifar10_record.image, tf.float32)\n",
    "  distorted_image = tf.random_crop(float_image, [height, width, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  distorted_image = tf.image.random_brightness(distorted_image, max_delta=63)\n",
    "  distorted_image = tf.image.random_contrast(\n",
    "      distorted_image, lower=0.2, upper=1.8)\n",
    "  # standardization: subtract off the mean and divide by the variance of the pixels\n",
    "  distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "  # Set the shapes of tensors.\n",
    "  distorted_image.set_shape([height, width, 3])\n",
    "  cifar10_record.label.set_shape([1])\n",
    "  # ensure a level of mixing of elements.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(\n",
    "      NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue)\n",
    "  # (8) example queue\n",
    "  # Filling queue with min_queue_examples CIFAR images before starting to train\n",
    "  image_batch, label_batch = tf.train.shuffle_batch(\n",
    "      [distorted_image, cifar10_record.label],\n",
    "      batch_size=batch_size,\n",
    "      num_threads=16,\n",
    "      capacity=min_queue_examples + 3 * batch_size,\n",
    "      min_after_dequeue=min_queue_examples)\n",
    "  return image_batch, tf.reshape(label_batch, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to generate the data for testing. Now, you are able to specify the order of input pipeline in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_input(testing_files, batch_size):\n",
    "  for f in testing_files:\n",
    "    if not tf.gfile.Exists(f):\n",
    "      raise ValueError('Failed to find file: ' + f)\n",
    "  # create a queue that produces filenames to read\n",
    "  file_queue = tf.train.string_input_producer(testing_files)\n",
    "  cifar10_record = read_cifar10(file_queue)\n",
    "  # image preprocessing for training\n",
    "  height = IMAGE_SIZE_CROPPED\n",
    "  width = IMAGE_SIZE_CROPPED\n",
    "  float_image = tf.cast(cifar10_record.image, tf.float32)\n",
    "  resized_image = tf.image.resize_image_with_crop_or_pad(\n",
    "      float_image, height, width)\n",
    "  image_eval = tf.image.per_image_standardization(resized_image)\n",
    "  image_eval.set_shape([height, width, 3])\n",
    "  cifar10_record.label.set_shape([1])\n",
    "  # Ensure that the random shuffling has good mixing properties.\n",
    "  min_fraction_of_examples_in_queue = 0.4\n",
    "  min_queue_examples = int(\n",
    "      NUM_EXAMPLES_PER_EPOCH_FOR_EVAL * min_fraction_of_examples_in_queue)\n",
    "  image_batch, label_batch = tf.train.batch(\n",
    "      [image_eval, cifar10_record.label],\n",
    "      batch_size=batch_size,\n",
    "      num_threads=16,\n",
    "      capacity=min_queue_examples + 3 * batch_size)\n",
    "  return image_batch, tf.reshape(label_batch, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the input pipeline, we can check the functionality of the example queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cropped image: (128, 24, 24, 3)\n",
      "Shape of label: (128,)\n"
     ]
    }
   ],
   "source": [
    "# test function distort_input\n",
    "with tf.Session() as sess:\n",
    "  coord = tf.train.Coordinator()\n",
    "  image, label = distort_input(training_files, BATCH_SIZE)\n",
    "  # --- Note ---\n",
    "  # If you forget to call start_queue_runners(), it will hang\n",
    "  # indefinitely and deadlock the user program.\n",
    "  # ------------\n",
    "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "  image_batch, label_batch = sess.run([image, label])\n",
    "  coord.request_stop()\n",
    "  coord.join(threads)\n",
    "  image_batch_np = np.asarray(image_batch)\n",
    "  label_batch_np = np.asarray(label_batch)\n",
    "  print('Shape of cropped image:', image.shape)\n",
    "  print('Shape of label:', label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have prepared input queues. Let's start designing our cnn model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tensorflow Dataset API\n",
    "\n",
    "From last lab, we briefly introduced `tf.data.Dataset` API for reading data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tensorflow recommended queue-base input pipeline before version 1.2. After with version 1.2, tensorflow recommend using the [tf.data](https://www.tensorflow.org/programmers_guide/datasets) instead. Read [more](https://github.com/tensorflow/tensorflow/issues/7951)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a data input pipeline with `tf.data.Dataset`, here are the steps for you can follow:\n",
    "1. Define data source and initialize your Dataset object\n",
    "2. Apply transformations on the dataset, following are some common useful techniques\n",
    "    - map\n",
    "    - batch\n",
    "    - shuffle\n",
    "    - repeat\n",
    "3. Create iterator \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Construct your Dataset \n",
    "First, we have to construct a Dataset object from our data source(some tensors in the memory, or data stored in the disk such as [TFRecord](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) format)\n",
    "\n",
    "Now suppose we have simple data sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an array with shape (8, 5)\n",
    "raw_data_a = np.array([[1, 1.1, 1.2, 1.3, 1.4], \n",
    "                     [2, 2.1, 2.2, 2.3, 2.4], \n",
    "                     [3, 3.1, 3.2, 3.3, 3.4],\n",
    "                     [4, 4.1, 4.2, 4.3, 4.4],\n",
    "                     [5, 5.1, 5.2, 5.3, 5.4],\n",
    "                     [6, 6.1, 6.2, 6.3, 6.4],\n",
    "                     [7, 7.1, 7.2, 7.3, 7.4],\n",
    "                     [8, 8.1, 8.2, 8.3, 8.4]\n",
    "                    ])\n",
    "# a list with len 8\n",
    "raw_data_b = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our tensorflow Dataset object with these two data using `tf.data.Dataset.from_tensor_slices`, which will automatically cut your data into slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((5,), ()), types: (tf.float64, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "# thie tells the dataset that each row of raw_data_a is corresponding to each element of raw_data_b\n",
    "dataset = tf.data.Dataset.from_tensor_slices((raw_data_a, raw_data_b))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Apply transformations\n",
    "Next, according to your needs, you can preprocess your data in this step.\n",
    "\n",
    "##### map\n",
    "For example, `Dataset.map()` provide element-wise customized data preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ((1, 5), ()), types: (tf.float64, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(one_row_a, one_b):\n",
    "    \"\"\"\n",
    "        Input: one slice of the dataset\n",
    "        Output: modified slice\n",
    "    \"\"\"\n",
    "    # do some data preprocessing\n",
    "    # you can also input filenames and load data in here\n",
    "    \n",
    "    return tf.reshape(one_row_a, [1, 5]), tf.string_to_number(one_b)\n",
    "\n",
    "dataset = dataset.map(preprocess_function)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shuffle\n",
    "`Dataset.shuffle` maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. This way, you can see your data coming with different order in different epoch. This can prevent your model overfit on the order of your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### batch\n",
    "Now our dataset is one example by one example. However, in reality, we usually want to read one batch at a time, thus we can call `Dataset.batch(batch_size)` to stack batch_size elements together.\n",
    "\n",
    "<b>Note:</b> Be careful that if you apply `Dataset.shuffle` after `Dataset.batch`, you'll get shuffled batch but data in a batch remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### repeat\n",
    "If you directly use the dataset above to form the iterator, you'll get `tf.errors.OutOfRangeError` after seeing all the data once(one epoch). `Dataset.repeat(epoch)` allow you iterate over a dataset in multiple epochs. `epoch = None` will let the dataset repeats indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Create Iterator and Read data\n",
    "The `tf.data` API currently supports the following iterators, in increasing level of sophistication:\n",
    "- one-shot : One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization.\n",
    "   \n",
    "   \n",
    "- initializable: requires you to run an explicit `iterator.initializer` operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more `tf.placeholder()` tensors that can be fed when you initialize the iterator.\n",
    "\n",
    "\n",
    "- reinitializable: can be initialized from multiple different Dataset objects. For example, if you have training data and testing data with same format.\n",
    "\n",
    "\n",
    "- feedable: can be used together with `tf.placeholder` to select what Iterator to use in each call to `tf.Session.run`.\n",
    "\n",
    "\n",
    "For more details and examples, please read in [tensorflow guide](https://www.tensorflow.org/guide/datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create one shot iterator and initializable iterator, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  [ 1.  7.]\n",
      "Batch  1 , b are  [ 2.  3.]\n",
      "Batch  2 , b are  [ 8.  4.]\n",
      "Batch  3 , b are  [ 6.  5.]\n",
      "Batch  4 , b are  [ 5.  7.]\n",
      "Batch  5 , b are  [ 4.  2.]\n",
      "Batch  6 , b are  [ 3.  8.]\n",
      "Batch  7 , b are  [ 1.  6.]\n"
     ]
    }
   ],
   "source": [
    "iterator_oneshot = dataset.make_one_shot_iterator()\n",
    "next_element_oneshot = iterator_oneshot.get_next()\n",
    "\n",
    "for i in range(8):\n",
    "    a, b = sess.run(next_element_oneshot)\n",
    "    print(\"Batch \", i, \", b are \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the example for initializable itarator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator_initializable = dataset.make_initializable_iterator()\n",
    "next_element_initializable = iterator_initializable.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# before getting the next element, should run the initializer\n",
    "sess.run(iterator_initializable.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  0 , b are  [ 6.  8.]\n",
      "Batch  1 , b are  [ 5.  1.]\n",
      "Batch  2 , b are  [ 7.  2.]\n",
      "Batch  3 , b are  [ 4.  3.]\n",
      "Batch  4 , b are  [ 4.  6.]\n",
      "Batch  5 , b are  [ 5.  7.]\n",
      "Batch  6 , b are  [ 3.  2.]\n",
      "Batch  7 , b are  [ 1.  8.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    a, b = sess.run(next_element_initializable)\n",
    "    print(\"Batch \", i, \", b are \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model for CIFAR 10\n",
    "### Model Structure\n",
    "<center><img style='width: 80%' src='imgsrc/model.png' /></center> \n",
    "### Model Details\n",
    "* We put all variables on CPU because we want GPU to only focus on calculation. \n",
    "* The cost function we use is simply the *cross entropy* of labels and predictions.\n",
    "* *Weight decay* is a very common regularization technique. For NNs, we can penalize large weights in the cost function. The implementation of weight decay is simple: add a term in the cost function that penalizes the $L^{2}$-norm of the weight matrix at each layer.\n",
    "$$\\operatorname{arg}\\underset{\\Theta=\\{\\boldsymbol{W^{(1)}}{\\cdots}\\boldsymbol{W^{(L)}}\\}}{\\operatorname{min}}C(\\Theta)+\\alpha\\sum_{i=1}^{L} \\lVert \\boldsymbol{W^{(i)}} \\rVert_{2}^{2}$$ \n",
    "* *Local response normalization* is mentioned in original [*AlexNet*](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf) article in NIPS 2012. Because the activation function we used in our CNN model is *ReLU*, whose output has no upper bound. Thus, we need a local response normalization to normalize that.  \n",
    "Denoting by $a_{x,y}^i$ the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity, the response-normalized activity $b^i_{x,y}$ is given by the expression \n",
    "$$ b^i_{x,y} = a^i_{x,y} / \\left( k + \\alpha \\sum_{j=max(0,i-n/2)}^{min(N-1, i+n/2)} (a^j_{x,y})^2 \\right)^\\beta$$ \n",
    "where the sum runs over $n$ **adjacent** kernel maps at the same spatial position, and $N$ is the total number of kernels in the layer. The ordering of the kernel maps is arbitrary and determined before training begins.  The constants $k$, $n$, $\\alpha$, and $\\beta$ are hyper-parameters. Check the following figure drawn by Hu Yixuan.\n",
    "<center><img style='width: 80%' src='imgsrc/localResponseNormalization.jpeg' /></center> \n",
    "* When using gradient descent to update the weights of a neural network, sometimes the weights might move in the wrong direction. Thus, we take a [moving average](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/moving_averages) of the weights over a bunch of previous updates.  \n",
    "  \n",
    "  $$\\boldsymbol{w_{avg_i}} = decay\\times\\boldsymbol{w_{avg_{i-1}}} + (1-decay)\\times\\boldsymbol{w_{i}}$$\n",
    "where $w_{i}$ is the $i_{th}$ updated weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_Model(object):\n",
    "  def __init__(self, model_hps):\n",
    "    self.image_size = model_hps.image_size\n",
    "    self.batch_size = model_hps.batch_size\n",
    "    self.num_classes = model_hps.num_classes\n",
    "    self.num_training_example = model_hps.num_training_example\n",
    "    self.num_epoch_per_decay = model_hps.num_epoch_per_decay\n",
    "    self.init_lr = model_hps.init_lr  # initial learn rate\n",
    "    self.moving_average_decay = model_hps.moving_average_decay\n",
    "    self.ckpt_dir = model_hps.ckpt_dir\n",
    "    \n",
    "    self.build_model()\n",
    "    \n",
    "  def build_model(self):\n",
    "    # op for training\n",
    "    self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "    with tf.variable_scope('model'):\n",
    "      self.images = tf.placeholder(tf.float32,[self.batch_size, self.image_size, self.image_size, 3]) \n",
    "      self.labels = tf.placeholder(tf.int32)\n",
    "        \n",
    "      self.logits = self.inference(self.images)\n",
    "      self.top_k_op = tf.nn.in_top_k(self.logits, self.labels, 1) \n",
    "      self.total_loss = self.loss(self.logits, self.labels)\n",
    "      self.train_op = self.train(self.total_loss, self.global_step)\n",
    "    \n",
    "  def _variable_on_cpu(self, name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "      var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    return var\n",
    "\n",
    "  def _variable_with_weight_decay(self, name, shape, stddev, wd=0.0):\n",
    "    \"\"\" Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with a truncated normal \n",
    "        distribution. A weight decay is added only if one is specified.\n",
    "        -----\n",
    "        Args:\n",
    "            name: \n",
    "                name of the variable\n",
    "            shape: \n",
    "                a list of ints\n",
    "            stddev: \n",
    "                standard deviation of a truncated Gaussian\n",
    "            wd: \n",
    "                add L2Loss weight decay multiplied by this float. If None, weight\n",
    "                decay is not added for this Variable.\n",
    "        Returns:\n",
    "            Variable Tensor\n",
    "    \"\"\"\n",
    "    initializer = tf.truncated_normal_initializer(\n",
    "        stddev=stddev, dtype=tf.float32)\n",
    "    var = self._variable_on_cpu(name, shape, initializer)\n",
    "    # deal with weight decay\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "  def _conv_block(self, inp, scope, kernel_width, kernel_height, inp_channel, out_channel, strides = [1, 1, 1, 1], padding='SAME'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "      kernel = self._variable_with_weight_decay('weights', [kernel_width, kernel_width, inp_channel, out_channel], 5e-2)\n",
    "      biases = self._variable_on_cpu('bias', [out_channel], tf.constant_initializer(0.0))\n",
    "\n",
    "      conv = tf.nn.conv2d(inp, kernel, strides=strides, padding=padding)\n",
    "      pre_activation = tf.nn.bias_add(conv, biases)\n",
    "      return tf.nn.relu(pre_activation, name=scope.name)\n",
    "\n",
    "  def _fully_connected_layer(self, inp, scope, in_dim, out_dim, relu = True):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "      weights = self._variable_with_weight_decay('weights', [in_dim, out_dim], 0.04, 0.004)\n",
    "      biases = self._variable_on_cpu('biases', [out_dim], tf.constant_initializer(0.1))\n",
    "      if relu:\n",
    "        return tf.nn.relu(tf.matmul(inp, weights) + biases, name=scope.name)\n",
    "      else:\n",
    "        return tf.matmul(inp, weights) + biases\n",
    "    \n",
    "  def inference(self, images):\n",
    "    \"\"\" build the model\n",
    "        -----\n",
    "        Args:\n",
    "            images with shape [batch_size,24,24,3]\n",
    "        Return:\n",
    "            logits with shape [batch_size,10]\n",
    "    \"\"\"\n",
    "    conv_1 = self._conv_block(images, 'conv_1', 5, 5, 3, 64)\n",
    "    # pool_1\n",
    "    pool_1 = tf.nn.max_pool(conv_1,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_1')\n",
    "    # norm_1 (local_response_normalization)\n",
    "    norm_1 = tf.nn.lrn(pool_1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_1')\n",
    "    \n",
    "    # conv2\n",
    "    conv_2 = self._conv_block(norm_1, 'conv_2', 5, 5, 64, 64)\n",
    "    # norm2\n",
    "    norm_2 = tf.nn.lrn(conv_2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm_2')\n",
    "    # pool2\n",
    "    pool_2 = tf.nn.max_pool(norm_2,ksize=[1, 3, 3, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool_2')\n",
    "    \n",
    "    # Flatten feature maps before fully connected layers\n",
    "    flat_features = tf.reshape(pool_2, [self.batch_size, -1])\n",
    "    dim = flat_features.get_shape()[1].value\n",
    "    # FC_1 (fully-connected layer)\n",
    "    fc_1 = self._fully_connected_layer(flat_features, 'fc1', dim, 384)\n",
    "\n",
    "    # FC_2\n",
    "    fc_2 = self._fully_connected_layer(fc_1, 'fc2', 384, 192)\n",
    "\n",
    "    logits = self._fully_connected_layer(fc_2, 'softmax_linear', 192, self.num_classes, relu = False)\n",
    "    return logits\n",
    "\n",
    "  def loss(self, logits, labels):\n",
    "    '''calculate the loss'''\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=labels, logits=logits, name='cross_entropy_per_example')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    tf.add_to_collection('losses', cross_entropy_mean)\n",
    "    # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "    # decay terms (L2 loss).\n",
    "    return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "\n",
    "  def train(self, total_loss, global_step):\n",
    "    '''Return training operation of one step'''\n",
    "    num_batches_per_epoch = self.num_training_example / self.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * self.num_epoch_per_decay)\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(\n",
    "        self.init_lr, global_step, decay_steps, decay_rate=0.1, staircase=True)\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    # This step just records the moving average weights but not uses them\n",
    "    ema = tf.train.ExponentialMovingAverage(self.moving_average_decay,\n",
    "                                            global_step)\n",
    "    self.ema = ema\n",
    "    variables_averages_op = ema.apply(tf.trainable_variables())\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "      train_op = tf.no_op(name='train')\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train our model. First, we need to feed some hyperparameters into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_hps_cifar = tf.contrib.training.HParams(\n",
    "  image_size = IMAGE_SIZE_CROPPED,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  num_classes = NUM_CLASSES,\n",
    "  num_training_example = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
    "  num_epoch_per_decay = 350.0,\n",
    "  init_lr = 0.1,\n",
    "  moving_average_decay = 0.9999,\n",
    "  ckpt_dir = './model/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# CNN model\n",
    "model = CNN_Model(model_hps_cifar)\n",
    "# Here we use CPU to handle the input because we want GPU to only focus on training.\n",
    "with tf.device('/cpu:0'):\n",
    "  data_train = distort_input(training_files, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our model 180 epochs and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(model, data_train , num_epoch):\n",
    "  saver = tf.train.Saver()\n",
    "  with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(model.ckpt_dir)\n",
    "    if (ckpt and ckpt.model_checkpoint_path):\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      # assume the name of checkpoint is like '.../model.ckpt-1000'\n",
    "      gs = int(ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1])\n",
    "      sess.run(tf.assign(model.global_step, gs))\n",
    "    else:\n",
    "      # no checkpoint found\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    model.loss_each_epoch = []\n",
    "    \n",
    "    num_batch_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN//model.batch_size\n",
    "    #start training\n",
    "    for i in range(num_epoch):\n",
    "      _loss = []\n",
    "      for _ in range(num_batch_per_epoch):\n",
    "        images, labels = sess.run(data_train)\n",
    "        \n",
    "        l, _ = sess.run([model.total_loss, model.train_op], feed_dict = {model.images:images, model.labels:labels})\n",
    "        _loss.append(l)\n",
    "      loss_this_epoch = np.sum(_loss)\n",
    "      gs = model.global_step.eval()\n",
    "      # print('loss of epoch %d: %f' % (gs / num_batch_per_epoch, loss_this_epoch))\n",
    "      model.loss_each_epoch.append(loss_this_epoch)\n",
    "      saver.save(sess, model.ckpt_dir + 'model.ckpt', global_step=gs)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "  print('Done training %d epochs' %num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training 180 epochs\n"
     ]
    }
   ],
   "source": [
    "run_training(model, data_train, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because now the weights are not moving average weights, we need to manually change this.  \n",
    "    \n",
    "    tf.train.ExponentialMovingAverage(decay).variables_to_restore()\n",
    "    \n",
    "gives us a dictionary about the mapping between the weights and the moving average shadow weights. \n",
    "    We can use this mapping to replace the original weights by moving average shadow weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_testing(model, data_test):\n",
    "  variables_to_restore = model.ema.variables_to_restore()\n",
    "  saver = tf.train.Saver(variables_to_restore)\n",
    "    \n",
    "  with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    ckpt = tf.train.get_checkpoint_state(model.ckpt_dir)\n",
    "        \n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "      coord = tf.train.Coordinator()\n",
    "      threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "      num_iter = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL // model.batch_size\n",
    "      total_sample_count = num_iter * model.batch_size\n",
    "      true_count = 0\n",
    "      for _ in range(num_iter):\n",
    "        images, labels = sess.run(data_test) \n",
    "        \n",
    "        predictions = sess.run(model.top_k_op, feed_dict = {model.images:images, model.labels:labels})\n",
    "        true_count += np.sum(predictions)\n",
    "      print('Accurarcy: %d/%d = %f' % (true_count, total_sample_count,\n",
    "                                         true_count / total_sample_count))\n",
    "      coord.request_stop()\n",
    "      coord.join(threads)\n",
    "    else:\n",
    "      print('train first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done our training! Let's see whether our model is great or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = distort_input(testing_files, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt-70200\n",
      "Accurarcy: 8226/9984 = 0.823918\n"
     ]
    }
   ],
   "source": [
    "run_testing(model, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a much higher accuracy than KNN and SVM. This is good enough!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Assignment\n",
    "Implement the input pipeline of the CNN model with [dataset](https://www.tensorflow.org/programmers_guide/datasets) API. The dataset should be multithreaded (16 threads). To simplify, you only need to train the model for 10 epochs. Finally, get the accuracy of this 10-epoch model. There are 4 'TODO' parts you need to finish. You only need to hand out the Lab12_{student_id}.ipynb.  \n",
    "The notebook should include \n",
    "* Training loss per epoch\n",
    "* The testing accuracy\n",
    "* The total time to train and test\n",
    "\n",
    "<b>Note:</b> since cifar10 dataset is in binary format, you should use [`tf.data.FixedLengthRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/FixedLengthRecordDataset), which is a class inherits from `tf.data.Dataset`.   \n",
    "\n",
    "Deadline: 2018/11/15 15:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "DEST_DIRECTORY = 'dataset/cifar10'\n",
    "DATA_DIRECTORY = DEST_DIRECTORY + '/cifar-10-batches-bin'\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "IMAGE_SIZE_CROPPED = 24\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10 \n",
    "LABEL_BYTES = 1\n",
    "IMAGE_BYTES = 32 * 32 * 3\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# download it\n",
    "maybe_download_and_extract(DEST_DIRECTORY, DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_record_distort_parser(record):\n",
    "  ''' Parse the record into label, cropped and distorted image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped and distorted image in the record.\n",
    "  '''\n",
    "  # TODO1\n",
    "  pass\n",
    "\n",
    "\n",
    "def cifar10_record_crop_parser(record):\n",
    "  ''' Parse the record into label, cropped image\n",
    "    -----\n",
    "    Args:\n",
    "        record: \n",
    "            a record containing label and image.\n",
    "    Returns:\n",
    "        label: \n",
    "            the label in the record.\n",
    "        image: \n",
    "            the cropped image in the record.\n",
    "  '''\n",
    "  # TODO2\n",
    "  pass\n",
    "\n",
    "\n",
    "def cifar10_iterator(filenames, batch_size, cifar10_record_parser):\n",
    "  ''' Create a dataset and return a tf.contrib.data.Iterator \n",
    "    which provides a way to extract elements from this dataset.\n",
    "    -----\n",
    "    Args:\n",
    "        filenames: \n",
    "            a tensor of filenames.\n",
    "        batch_size: \n",
    "            batch size.\n",
    "    Returns:\n",
    "        iterator: \n",
    "            an Iterator providing a way to extract elements from the created dataset.\n",
    "        output_types: \n",
    "            the output types of the created dataset.\n",
    "        output_shapes: \n",
    "            the output shapes of the created dataset.\n",
    "  '''\n",
    "\n",
    "  # TODO3\n",
    "  # tips: use dataset.map with cifar10_record_parser(record)\n",
    "  #       output_types = dataset.output_types\n",
    "  #       output_shapes = dataset.output_shapes\n",
    "\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_hps_cifar = tf.contrib.training.HParams(\n",
    "  image_size = IMAGE_SIZE_CROPPED,\n",
    "  batch_size = BATCH_SIZE,\n",
    "  num_classes = NUM_CLASSES,\n",
    "  num_training_example = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN,\n",
    "  num_epoch_per_decay = 350.0,\n",
    "  init_lr = 0.1,\n",
    "  moving_average_decay = 0.9999,\n",
    "  ckpt_dir = './model/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define training/testing data files\n",
    "training_files = [os.path.join(DATA_DIRECTORY, 'data_batch_%d.bin' % i) for i in range(1, 6)]\n",
    "testing_files = [os.path.join(DATA_DIRECTORY, 'test_batch.bin')]\n",
    "filenames_train = tf.constant(training_files)\n",
    "filenames_test = tf.constant(testing_files)\n",
    "\n",
    "# Training data iterator\n",
    "iterator_train, types, shapes = cifar10_iterator(filenames_train, BATCH_SIZE, cifar10_record_distort_parser)\n",
    "# Testing data iterator\n",
    "iterator_test, _, _ = cifar10_iterator(filenames_test, BATCH_SIZE, cifar10_record_crop_parser)\n",
    "\n",
    "# use to handle training and testing\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, types, shapes)\n",
    "labels_images_pairs = iterator.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "model = CNN_Model(model_hps_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO4:\n",
    "# 1. train the CNN model 10 epochs\n",
    "# 2. show the loss per epoch\n",
    "# 3. get the accuracy of this 10-epoch model\n",
    "# 4. measure the time using '%%time' instruction\n",
    "# tips:\n",
    "# use placeholder handle to determine if training or testing. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
